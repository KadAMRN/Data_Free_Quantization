
\begin{abstract}

\textbf{Le résumé  synthétise en environ 200 mots la tâche abordées, les problèmes associés, la solution proposée, et les résultats principaux} \\

This report delves into the implementation of quantization techniques aimed at optimizing Deep Neural Networks (DNNs) for enhanced inference performance. By quantizing neural network models, we effectively reduce their storage footprint and leverage integer arithmetic for faster computation, capitalizing on the integer math pipelines of modern processors. We present a comprehensive review of the mathematical underpinnings of quantization parameters and their impact on a diverse set of DNN architectures across various domains such as image processing, speech recognition, and natural language processing. Special emphasis is placed on quantization approaches that seamlessly integrate with and exploit the capabilities of high-throughput integer units, thus facilitating efficient hardware acceleration. Through a series of experiments, we assess the influence of different quantization strategies on model accuracy and computational efficiency, offering insights into their practical applicability. Our findings suggest that intelligent quantization can significantly expedite DNN inference without compromising on model fidelity, thereby serving as a pivotal step towards deploying AI in resource-constrained environments.
\end{abstract}

\begin{keywords}
Data-free quantization, Bias correction, 8-bit fixed-point quantization, Deep neural networks
\end{keywords}
